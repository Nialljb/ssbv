{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Subject Brain Volumetric Analysis \n",
    "### Beta: run local Jupyter notebook that can submit jobs remotely to the HPC\n",
    "*nbourke@imperial.ac.uk March 2022* \n",
    "\n",
    "## Overview\n",
    "This notebook takes a set of scans and tells you what the volumes of grey matter, white matter and CSF are. It also lets you run a voxelwise comparison (voxel based morphometry) between the two groups (patients versus controls) to see whether the volume at each voxel is significantly different. Eg. Do patients have smaller brains (ie are they more atrophic) than controls?\n",
    "\n",
    "The notebook uses SPM12 (UCL) to do most of the heavy lifting, and then FSL for the stats. You might find the SPM manual useful: https://www.fil.ion.ucl.ac.uk/spm/doc/manual.pdf.\n",
    "\n",
    "Matlab dependencies by Neil Graham and Greg Scott\n",
    "\n",
    "#### Development notes:\n",
    "- Could/should take out dependency of shell scripting in Jupyter. Each of these BASH cells could be shell scripts called from lib. This would simplify the workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "You will need: \n",
    "\n",
    "-Data stored in BIDS format  \n",
    "\n",
    "* hpcwrapmatlab.sh\n",
    "* hpcrunarrayjob.sh\n",
    "* segment_t1.m\n",
    "* make_template.m\n",
    "* generate_flowfields.m\n",
    "* move_to_mni.m\n",
    "\n",
    "**All of these scripts should be in the dependencies folder now**\n",
    "\n",
    "### Setup\n",
    "- Requires local installation of Jupyter (with desired env setup)\n",
    "    This can be done via conda\n",
    "- Expects remote file system mounted as a volume (e.g. macFuse https://osxfuse.github.io/)\n",
    "- Local installations of software (e.g. fsl, mrtrix)\n",
    "- A login file is required that contains your ssh login details\n",
    "- This notebook should sit on the cluster\n",
    "\n",
    "#### Setup: Mounting remote file system \n",
    "1) install macFuse & sshfs https://osxfuse.github.io/\n",
    "2) save the below commands into your .zshrc or .bash_profile depending on the shell you use:\n",
    "```bash\n",
    "function mount_cluster_home () { if [ ! -e /Users/<USERNAME>/hpc ]; then mkdir /Users/<USERNAME>/hpc;fi ; sshfs <USERNAME>@login.hpc.ic.ac.uk:/home/<USERNAME>/ /Users/<USERNAME>/hpc -p 22 -o follow_symlinks;}\n",
    "alias mounthpc='diskutil unmount /Users/<USERNAME>/hpc ; mkdir /Users/<USERNAME>/hpc ; sshfs <USERNAME>@login.hpc.ic.ac.uk:/home/<USERNAME>/ /Users/<USERNAME>/hpc -p 22 -o follow_symlinks ; cd /Users/<USERNAME>/hpc ; ls'  \n",
    "```  \n",
    "   \n",
    "3) now you should be able to mount the cluster by typing _hpc_ into terminal \n",
    "\n",
    "#### Setup: Login file\n",
    "    \n",
    "> nano ~/.activate_imperial_rcs_login \n",
    "\n",
    "```bash\n",
    "# Setup SSH connection (add to profile?)\n",
    "## Export DISPLAY\n",
    "export DISPLAY=:0; \n",
    "export SSH_ASKPASS \n",
    "\n",
    "## Export command as env variable so can be called in localSubmit script\n",
    "export cluster=\"ssh <USERNAME>@login.hpc.ic.ac.uk\" \n",
    "export chome=\"/rds/general/user/<USERNAME>/home\"\n",
    "\n",
    "## Passphrase \n",
    "echo \"echo <PASSWORD>\" > /tmp/ask\n",
    "ask=\"/tmp/ask\"\n",
    "chmod +x $ask\n",
    "SSH_ASKPASS=$ask\n",
    "```\n",
    "\n",
    "#### Setup: symbolic links on cluster\n",
    "\n",
    "By default when you mount you will be able to see your own home directory. However you may want access to project or ephermeral space. This can be achieved by setting up symbolic links on the cluster.\n",
    "\n",
    "> ln -s source_file symbolic_link\n",
    "\n",
    "When you mount the cluster you will have access to these links\n",
    "\\* Permissions still apply, so specific directories may be granted to work on remotely but *not* top level project directories \n",
    "\n",
    "I have set ephermeral to \n",
    "> ~/eph\n",
    "\n",
    "and this is the space I tend to work  \n",
    "\n",
    "Permissions may need to be granted to start working on a file or folder\n",
    "\n",
    "> chmod +wrx <file/folder>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of volumes from T1 sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's define some paths and get FSL loaded etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining notebook variables and paths\n",
    "##### python cell\n",
    "    1. enter project ID\n",
    "    2. If this project doesn't exist in in the temporary space, create it\n",
    "    3. Set importnat paths that will be used\n",
    "    4. Make a setup script to save bash variables in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "\n",
    "# Define project name\n",
    "project = \"pcnorad\" \n",
    "\n",
    "# Set project dir in ephermeral \n",
    "directory = (home + \"/hpc/eph/\" + project + \"/data/\")\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "workingDir = (home + \"/hpc/eph/\" + project + \"/\")\n",
    "wd = (home + \"/eph/\" + project + \"/\")\n",
    "setup = (workingDir + \"/setup.sh\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### bash cell\n",
    "This cell adds bash variables you want to save to a setup script, which can then be called in future python cells. \n",
    "\n",
    "1. Add project name\n",
    "2. Add paths of interest\n",
    "3. Define modules that will be needed\n",
    "\n",
    "### Symbolic links can be created to specific shared folders with appropriate permissions, but not top level project directories.\n",
    "\n",
    "> ln -s /rds/general/project/c3nl_djs_imaging_data/live/data ~/imData \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $setup\n",
    "\n",
    "$(: Project label) \n",
    "project=\"pcnorad\"\n",
    "\n",
    "$(: dependencies)\n",
    "export repo=pwd\n",
    "export live=${repo}/lib\n",
    "export dep=/rds/general/project/c3nl_shared/live/dependencies\n",
    "\n",
    "export workingDir=~/hpc/eph/${project}\n",
    "\n",
    "wd=/rds/general/user/nbourke/ephemeral/${project}\n",
    "# imData is a symbolic link to shared directory\n",
    "imData=~/hpc/imData\n",
    "raw=${imData}/raw\n",
    "\n",
    "# source login details\n",
    "source ~/.activate_imperial_rcs_login\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$setup\"\n",
    "export setup=$1;\n",
    "source $setup\n",
    "echo $workingDir\n",
    "pwd\n",
    "\n",
    "echo ${ask}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy data\n",
    "\n",
    "Copy data in BIDS format from source directory into a temporary working directory\n",
    "- for a project downloaded with the xnat downloader script and converted to bids format you can index by looping over the raw project folder and then copying the participants from the sourcedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$setup\"\n",
    "export setup=$1;\n",
    "source $setup\n",
    "echo $workingDir\n",
    "# ----------------\n",
    "\n",
    "# Index subjects by raw project directory - find BIDS format scans from sourcedata\n",
    "for sub in `ls ${imData}/raw/${project}`; \n",
    "    do\n",
    "    for ses in `ls ${imData}/sourcedata/sub-${sub}`;\n",
    "        do\n",
    "        #echo ${ses}\n",
    "        mkdir -p ${workingDir}/data/sub-${sub}/${ses}/anat/T1w/\n",
    "        tmp=`ls ${imData}/sourcedata/sub-${sub}/${ses}/anat/T1w/*T1w*`\n",
    "        echo $tmp\n",
    "        cp ${tmp} ${workingDir}/data/sub-${sub}/${ses}/anat/T1w/\n",
    "        gunzip ${workingDir}/data/sub-${sub}/${ses}/anat/T1w/*\n",
    "    done        \n",
    "done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Data validation is required to check for new/missed participants not copied and to control for individuals with multiple scanning sessions*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a list of T1 scans\n",
    "- This is where things can get confusing between local and remote root paths\n",
    "- workingDir is defined above as the root path for your local machine\n",
    "- wd is defined above as the root path for the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$setup\"\n",
    "export setup=$1;\n",
    "source ${setup}\n",
    "echo ${workingDir}\n",
    "\n",
    "# module load fsl\n",
    "#----------------------------\n",
    "\n",
    "# setup log dir\n",
    "if [ ! -d ./logs/ ]; then\n",
    "    echo \"Making log dir!\"\n",
    "    mkdir -p ./logs/\n",
    "fi\n",
    "#\n",
    "\n",
    "# Make a list of all T1 scans\n",
    "echo -n \"\" > ./logs/${project}_t1_list.txt\n",
    "for s in `ls -d ${workingDir}/data/*`;\n",
    "    do\n",
    "    subj=`basename ${s}`\n",
    "    for ses in `ls ${workingDir}/data/${subj}`; \n",
    "        do \n",
    "        echo \"${wd}\"/data/${subj}/${ses}/anat/T1w/${subj}_${ses}_T1w.nii >> ./logs/${project}_t1_list.txt\n",
    "    done\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Run segmentation jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$setup\"\n",
    "export setup=$1;\n",
    "source ${setup}\n",
    "echo ${workingDir}\n",
    "#----------------------------\n",
    "\n",
    "# Segment T1 data\n",
    "echo -n \"\" > ./logs/${project}_segmentationJobs.txt\n",
    "job=./logs/${project}_segmentationJobs.txt\n",
    "\n",
    "for subject in `cat ./logs/${project}_t1_list.txt`\n",
    "    do\n",
    "    echo \"${dep}/hpcwrapmatlab.sh \\\"maxNumCompThreads(3); segment_t1('${subject}');\\\"\" >> ${job}  \n",
    "done;\n",
    "\n",
    "    # Run job\n",
    "    ~/hpc/repos/ssbv/lib/localSubmit ${job} 01:00:00 3 6Gb\n",
    "    echo \"\"; echo \"***\"; echo \"\"; echo \"Submitted commands:\"; head ${job}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: check on your job progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qstat -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After the job has completed look at the output:\n",
    "\n",
    "Four each subject you should have the following files:\n",
    "* subject....nii - this is the original untouched nifti - we could later delete it from here as it is stored in the sourcedata folder, in order to save space \n",
    "* c1 ....   - this is the grey matter segmented output\n",
    "* c2 ....   - this is the white matter segmented output\n",
    "* c3 ....   - this is the CSF segmented output\n",
    "* rc1 ... rc2  etc.. - this is a rigidly aligned GM segmented output (useful for later when we want to move files to 'standard space' such as MNI)\n",
    "* seg8 - has details of the segmentation to save SPM time if the software needs to reference the files later on\n",
    "\n",
    "And most importantly:\n",
    "* ...... vols.txt - this has your tissue volumes in it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vital step: Have a look at your scans to make sure the segmentation has worked properly in each case\n",
    "We can even generate some commands for you to use in the terminal with FSL.\n",
    "\n",
    "These are designed so it will be as painless as possible. Load up the terminal, connect to the HPC, make sure you do module load fsl\n",
    "\n",
    "1. Copy this cell into terminal to run all subjects (quit by pressing ctrl+c in terminal)\n",
    "2. Run this cell to get commands to copy into terminal to run one at a time\n",
    "\n",
    "*You want to ensure that the wm and gm are separated nicely and in a way which you think is appropriate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fn = dir('/rds/general/user/nbourke/ephemeral/fa/*.gz');\n",
    "\n",
    "# % Now we want to view as a movie for QA purposes. \n",
    "# figure;ax = gca;\n",
    "# % use the following to force the Current figure handle to appear outside the live script\n",
    "# set(gcf,'Visible','on')\n",
    "# for ii=1:numel(fn)\n",
    "#     plotNifti([fn(ii).folder,filesep,fn(ii).name],ax);\n",
    "#     drawnow % will tell Matlab to create animation\n",
    "#     pause(0.2) % how long to pause between loading the next image\n",
    "# end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can bundle up all of the vols.txt files into a big CSV for convenience, and put this in your notebook folder (within a subfolder called volumetric_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$setup\"\n",
    "export setup=$1;\n",
    "source $setup\n",
    "echo $workingDir\n",
    "#------------------\n",
    "\n",
    "if [ -d ${workingDir}/volumetric_results ]   \n",
    "    then\n",
    "    echo \"results folder ready\";\n",
    "    else\n",
    "    mkdir -p ${workingDir}/volumetric_results\n",
    "    echo \"results folder made\";\n",
    "fi\n",
    "   \n",
    "    echo \"subject,gm_vol,wm_vol,csf_vol\" > ${workingDir}/volumetric_results/volumes.csv \n",
    "    for subject in `ls ${workingDir}/data/`\n",
    "        do\n",
    "        for ses in `ls ${workingDir}/data/${subject}/`; \n",
    "            do \n",
    "            volsfile=${workingDir}/data/$subject/${ses}/anat/T1w/*_vols.txt       \n",
    "            if [ -f ${volsfile} ]\n",
    "               then\n",
    "               echo -n \"${ses}\" >> ${workingDir}/volumetric_results/volumes.csv;\n",
    "               tail -n 1 ${volsfile} >> ${workingDir}/volumetric_results/volumes.csv;\n",
    "            fi\n",
    "        done\n",
    "    done;\n",
    "    \n",
    "echo \"done\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could download these CSVs into the analysis package of your choice and do some comparisons using the summary measures.\n",
    "\n",
    "Eg. t-test comparing the GM volume in patients versus controls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voxelwise statistics \n",
    "### (and steps to get the images in standard space to facilitate this)\n",
    "In order to do comparisons on the shapes of different brains they need to be moved into 'standard space' such as MNI. SPM can do this for us using 'DARTEL', an approach which preserves volume information on moving.\n",
    "\n",
    "Then we can use FSL randomise to do voxelwise comparisons between groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2. First make a 'study-specific' template\n",
    "This is an average image of your subjects. Rather than going straight to standard space like MNI, it's better to go via a template. You could use all your subjects for this, or a selection of them. Ideally it should be 50% patients 50% controls.  Run the next cell to make a file listing which subjects to use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can take a while so you can increase the number to something more generous if you've got lots of subjects.  \n",
    "n=152 is a good maximum heuristic ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$setup\"\n",
    "export setup=$1;\n",
    "source ${setup}\n",
    "echo ${workingDir}\n",
    "#------------------\n",
    "\n",
    "# module load fsl\n",
    "#----------------------------\n",
    "dep=/rds/general/project/c3nl_shared/live/dependencies\n",
    "\n",
    "# this uses the RC rigidly aligned files from the segmentation output : if you want to have a look at them use this in bash  \n",
    "echo -n \"\" > ./logs/${project}_templateJob.txt\n",
    "job=./logs/${project}_templateJob.txt\n",
    "\n",
    "    files=\"\"\n",
    "  \n",
    "echo \"\" > ./logs/${project}_subj.txt   \n",
    "\n",
    "for subject in `ls ${workingDir}/data/`\n",
    "    do\n",
    "    for ses in `ls ${workingDir}/data/${subject}/`; \n",
    "        do \n",
    "         echo ${subject} >> ./tmp/subj.txt   \n",
    "        trc1=`basename ${workingDir}/data/${subject}/${ses}/anat/T1w/rc1*T1w.nii`;\n",
    "        trc2=`basename ${workingDir}/data/${subject}/${ses}/anat/T1w/rc2*T1w.nii`;\n",
    "        trc3=`basename ${workingDir}/data/${subject}/${ses}/anat/T1w/rc3*T1w.nii`;\n",
    "        rc1=${wd}/data/${subject}/${ses}/anat/T1w/${trc1}\n",
    "        rc2=${wd}/data/${subject}/${ses}/anat/T1w/${trc2}\n",
    "        rc3=${wd}/data/${subject}/${ses}/anat/T1w/${trc3}\n",
    "        \n",
    "        if [ -z \"${files}\" ]; then\n",
    "         files=\"'${rc1}','${rc2}','${rc3}'\";\n",
    "        else\n",
    "         files=\"${files},'${rc1}','${rc2}','${rc3}'\";\n",
    "        fi\n",
    "    done\n",
    "done\n",
    "     \n",
    "     echo \"${dep}/hpcwrapmatlab.sh \\\"maxNumCompThreads(3); make_template('Template', ${files});\\\"\" > ${job};\n",
    "              \n",
    "\n",
    "    # Setup SSH connection (add to profile)\n",
    "\n",
    "    # Run job\n",
    "    ~/hpc/repos/ssbv/lib/localSubmit ${job} 48:00:00 3 6Gb\n",
    "    echo \"\"; echo \"***\"; echo \"\"; echo \"Submitted commands:\"; head ${job}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the job is done, you need to move the completed template files to a nice new folder, as by default they are dumped into the **first** subject's folder \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$setup\"\n",
    "export setup=$1;\n",
    "source ${setup}\n",
    "#echo ${workingDir}\n",
    "#------------------\n",
    "\n",
    " # line=$(head -n 2 ./tmp/subj.txt) \n",
    " # echo $line\n",
    "\n",
    "# I don't know why this isn't working, and need to hardcode it..\n",
    "\n",
    "subjs=`ls -d ${workingDir}/data/*/ `; \n",
    "sub=\"${subjs[0]}\"; \n",
    "echo $sub\n",
    "\n",
    "# mkdir ${workingDir}/DARTEL_template\n",
    "cp ${workingDir}/data/sub-CIF0381/*/anat/T1w/Template_* ${workingDir}/DARTEL_template/;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Make flowfields to the newly made group template for each subject's scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$setup\"\n",
    "export setup=$1;\n",
    "source $setup\n",
    "echo $workingDir\n",
    "#------------------\n",
    "    \n",
    "template_basename=\"Template\"\n",
    "template=${wd}/DARTEL_template/${template_basename}\n",
    "\n",
    "unset files;\n",
    "echo \"\" > ./logs/${project}_flowfields.txt\n",
    "job=./logs/${project}_flowfields.txt\n",
    "\n",
    "for subject in `ls ${workingDir}/data/`\n",
    "    do\n",
    "    for ses in `ls ${workingDir}/data/${subject}/`; \n",
    "        do \n",
    "\n",
    "        files=\"\"\n",
    "    \n",
    "        trc1=`basename ${workingDir}/data/${subject}/${ses}/anat/T1w/rc1*T1w.nii`;\n",
    "        trc2=`basename ${workingDir}/data/${subject}/${ses}/anat/T1w/rc2*T1w.nii`;\n",
    "        trc3=`basename ${workingDir}/data/${subject}/${ses}/anat/T1w/rc3*T1w.nii`;\n",
    "        rc1=${wd}/data/${subject}/${ses}/anat/T1w/${trc1}\n",
    "        rc2=${wd}/data/${subject}/${ses}/anat/T1w/${trc2}\n",
    "        rc3=${wd}/data/${subject}/${ses}/anat/T1w/${trc3}\n",
    "        \n",
    "        files=\"'${rc1}','${rc2}','${rc3}'\";\n",
    "\n",
    "        if [ -f ${workingDir}/data/${subject}/${ses}/anat/T1w/$trc1 ];\n",
    "        then\n",
    "\n",
    "        echo \"${dep}/hpcwrapmatlab.sh \\\"generate_flowfields('${template}', ${files})\\\"\" >> ${job}; \n",
    "\n",
    "        else\n",
    "        echo \"No rc1 file for ${subject} at visit ${visit}\";\n",
    "        fi\n",
    "\n",
    "        unset files;\n",
    "    done\n",
    "done\n",
    "         \n",
    "    # Run job\n",
    "    ~/hpc/repos/ssbv/lib/localSubmit ${job} 12:00:00 3 6Gb\n",
    "    echo \"\"; echo \"***\"; echo \"\"; echo \"Submitted commands:\"; head ${job}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Use the flowfields to send the images to MNI space\n",
    "\n",
    "This uses smoothing with an 8mm gaussian kernel. This is reasonable...\n",
    "It uses the 'preserve volumes' option, whereby when a voxel is grown/expanded in the move to MNI, its value its reduced  (ie. the concentration of that voxel is modulated).\n",
    "\n",
    "If you need to change these settings edit the script move_to_mni.m and then re run your cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$setup\"\n",
    "export setup=$1;\n",
    "source ${setup}\n",
    "echo ${workingDir}\n",
    "#------------------\n",
    "       \n",
    "template_basename=\"Template\"\n",
    "template=${wd}/DARTEL_template/${template_basename}_6.nii\n",
    "\n",
    "\n",
    "unset files;\n",
    "echo \"\" > ./logs/${project}_register2MNI.txt;\n",
    "job=./logs/${project}_register2MNI.txt;\n",
    "\n",
    "\n",
    "for subject in `ls ${workingDir}/data/`\n",
    "    do\n",
    "    for ses in `ls ${workingDir}/data/${subject}/`; \n",
    "        do \n",
    "\n",
    "        files=\"\"\n",
    "\n",
    "        tu_rc1=`basename ${workingDir}/data/${subject}/${ses}/anat/T1w/u_rc1*_T1w.nii`;\n",
    "        tc1=`basename ${workingDir}/data/${subject}/${ses}/anat/T1w/c1*_T1w.nii`;\n",
    "        tc2=`basename ${workingDir}/data/${subject}/${ses}/anat/T1w/c2*_T1w.nii`;\n",
    "        tc3=`basename ${workingDir}/data/${subject}/${ses}/anat/T1w/c3*_T1w.nii`;\n",
    "        \n",
    "        u_rc1=${wd}/data/${subject}/${ses}/anat/T1w/${tu_rc1}\n",
    "        c1=${wd}/data/${subject}/${ses}/anat/T1w/${tc1}\n",
    "        c2=${wd}/data/${subject}/${ses}/anat/T1w/${tc2}\n",
    "        c3=${wd}/data/${subject}/${ses}/anat/T1w/${tc3}\n",
    "        \n",
    "        files=\"'${u_rc1}','${c1}','${c2}','${c3}'\";\n",
    "\n",
    "\n",
    "        if [ -f ${workingDir}/data/${subject}/${ses}/anat/T1w/$tu_rc1 ];\n",
    "        then\n",
    "        echo \"${dep}/hpcwrapmatlab.sh \\\"move_to_mni('${template}', ${files})\\\"\" >> ${job};\n",
    "        else\n",
    "        echo \"No flowfield for this person ${subject} and timepoint ${session}\";\n",
    "        fi\n",
    "\n",
    "        unset files;\n",
    "    done\n",
    "done\n",
    "         \n",
    "    # Run job\n",
    "    ~/hpc/repos/ssbv/lib/localSubmit ${job} 48:00:00 3 6Gb\n",
    "    echo \"\"; echo \"***\"; echo \"\"; echo \"Submitted commands:\"; head ${job}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$setup\"\n",
    "export setup=$1;\n",
    "source $setup\n",
    "echo $workingDir\n",
    "${fsl}\n",
    "#----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QC CHECK: Have a look at your MNI space spatially normalised and smoothed Jacobian images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Some tract stats in fsl\n",
    "* currently run locally in jupyter - can take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$setup\"\n",
    "export setup=$1;\n",
    "source ${setup}\n",
    "echo ${workingDir}\n",
    "${fsl}\n",
    "#----------------\n",
    "\n",
    "dataDir=${workingDir}/volumetric_results\n",
    "mkdir -p ${dataDir}/tractStats\n",
    "tractDIR=~/hpc/templates/Corrected_Tracts_MNI1mm/ # change to group templates\n",
    "\n",
    "# List standard GM\n",
    "echo \"\" > ${workingDir}/smwc1_list.txt\n",
    "\n",
    "for subject in `ls ${workingDir}/data/`\n",
    "    do\n",
    "    for ses in `ls ${workingDir}/data/${subject}/`; \n",
    "        do \n",
    "        ls ${workingDir}/data/${subject}/${ses}/anat/T1w/smwc1*_T1w.nii >> ${workingDir}/smwc1_list.txt\n",
    "    done\n",
    "done\n",
    "    \n",
    "\n",
    "# list standard WM\n",
    "echo \"\" > ${workingDir}/smwc2_list.txt\n",
    "for subject in `ls ${workingDir}/data/`\n",
    "    do\n",
    "    for ses in `ls ${workingDir}/data/${subject}/`; \n",
    "        do \n",
    "        ls ${workingDir}/data/${subject}/${ses}/anat/T1w/smwc2*_T1w.nii >> ${workingDir}/smwc2_list.txt\n",
    "    done\n",
    "done\n",
    "    \n",
    "    \n",
    "fslmerge -t ${dataDir}/all_smwc1 `cat ${workingDir}/smwc1_list.txt`\n",
    "fslmerge -t ${dataDir}/all_smwc2 `cat ${workingDir}/smwc2_list.txt`\n",
    "\n",
    "# create mean GM\n",
    "fslmaths ${dataDir}/all_smwc1 -max 0 -Tmin -bin ${dataDir}/all_smwc1_mask -odt char\n",
    "fslmaths ${dataDir}/all_smwc1 -mas ${dataDir}/all_smwc1_mask ${dataDir}/all_smwc1\n",
    "fslmaths ${dataDir}/all_smwc1 -Tmean ${dataDir}/mean_smwc1\n",
    "fslmaths ${dataDir}/mean_smwc1 -thr 0.5 -bin ${dataDir}/mean_smwc1_mask \n",
    "\n",
    "# create mean WM\n",
    "fslmaths ${dataDir}/all_smwc2 -max 0 -Tmin -bin ${dataDir}/all_smwc2_mask -odt char\n",
    "fslmaths ${dataDir}/all_smwc2 -mas ${dataDir}/all_smwc2_mask ${dataDir}/all_smwc2\n",
    "fslmaths ${dataDir}/all_smwc2 -Tmean ${dataDir}/mean_smwc2\n",
    "fslmaths ${dataDir}/mean_smwc2 -thr 0.5 -bin ${dataDir}/mean_smwc2_mask \n",
    "\n",
    "\n",
    "#Find the tract masks\n",
    "cd ${tractDIR}\n",
    "TRACTS=`ls *.gz`\n",
    "\n",
    "# Nested for loop - for each mask and each metric do fslstats\n",
    "for i in $TRACTS; \n",
    "do\n",
    "    #j=$(echo ${i} | cut -d '_' -f2-)\n",
    "    k=$(echo ${i} | cut -d '.' -f1)\n",
    "    \n",
    "    echo $k > ${dataDir}/tractStats/vol_MNI_${k}.txt\n",
    "    fslstats -t ${dataDir}/all_smwc2.nii.gz -k ${tractDIR}/$i -M >> ${dataDir}/tractStats/vol_MNI_${k}.txt\n",
    "done \n",
    "\n",
    "cp ${workingDir}/smwc2_list.txt ${dataDir}/tractStats/aaa.txt\n",
    "\n",
    "echo \"WM_VOL\" > ${dataDir}/tractStats/vol_MNI_WM.txt\n",
    "fslstats -t ${dataDir}/all_smwc2.nii.gz -k ${dataDir}/mean_smwc2_mask.nii.gz -M >> ${dataDir}/tractStats/vol_MNI_WM.txt \n",
    "echo \"GM_VOL\" > ${dataDir}/tractStats/vol_MNI_GM.txt\n",
    "fslstats -t ${dataDir}/all_smwc1.nii.gz -k ${dataDir}/mean_smwc1_mask.nii.gz -M >> ${dataDir}/tractStats/vol_MNI_GM.txt\n",
    "\n",
    "\n",
    "# paste -d , `ls` >> ../tractStats.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grey matter ROIs (Harvard-Oxford atlas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$setup\"\n",
    "export setup=$1;\n",
    "source ${setup}\n",
    "echo ${workingDir}\n",
    "#------------------\n",
    "\n",
    "# set output dir\n",
    "dataDir=${workingDir}/volumetric_results\n",
    "mkdir -p ${dataDir}/roiStats\n",
    "\n",
    "# source atlas\n",
    "atlasDIR=~/hpc/templates/HarvardOxford-atlas/\n",
    "\n",
    "# set job\n",
    "for brain in cortical subcortical; \n",
    "    do\n",
    "    cd ${atlasDIR}/${brain}/masks/\n",
    "    ROI=`ls *nii.gz`\n",
    "\n",
    "    # Nested for loop - for each mask and each metric do fslstats\n",
    "    for i in ${ROI}; \n",
    "        do\n",
    "        k=$(echo ${i} | cut -d '.' -f1); \n",
    "        echo $k > ${dataDir}/roiStats/vol_MNI_${k}.txt;\n",
    "        #echo \"fslstats -t ${dataDir}/all_smwc1.nii.gz -k ${atlasDIR}/${brain}/masks/${i} -M -V | awk {'print \\$1 * \\$3'} >> ${dataDir}/roiStats/vol_MNI_${k}.txt\" >> ${job}  \n",
    "        fslstats -t ${dataDir}/all_smwc1.nii.gz -k ${atlasDIR}/${brain}/masks/${i} -M >> ${dataDir}/roiStats/MI_MNI_${k}.txt\n",
    "    done\n",
    "done \n",
    "       \n",
    "# paste -d , `ls` >> ../tractStats.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Remember to copy relevent outputs to derivitives! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Not yet ammended the following cells for local working  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voxelwise analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$setup\"\n",
    "export setup=$1;\n",
    "source ${setup}\n",
    "echo ${workingDir}\n",
    "${fsl}\n",
    "#----------------\n",
    "\n",
    "# Update working dir, as done after main analysis was conducted\n",
    "wd=/rds/general/project/c3nl_djs_imaging_data/live/analysis/paeds\n",
    "\n",
    "# Remove pilot\n",
    "fslmerge -t ${wd}/derivatives/volumetric_results/all_smwc1 `cat ${wd}/derivatives/volumetric_results/smwc1_list.txt`\n",
    "fslmerge -t ${wd}/derivatives/volumetric_results/all_smwc2 `cat ${wd}/derivatives/volumetric_results/smwc2_list.txt`\n",
    "\n",
    "# design=${scriptDir}/scripts/design/demo.mat\n",
    "# contrast=${scriptDir}/scripts/design/mainContrast.con \n",
    "# setup_masks ${design} ${contrast} ${scriptDir}/scripts/design/${ii} `cat ${scriptDir}/scripts/design/masks.txt` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$setup\"\n",
    "export setup=$1;\n",
    "source ${setup}\n",
    "echo ${workingDir}\n",
    "module load fsl/6.0.1/\n",
    "#----------------------------\n",
    "\n",
    "wd=/rds/general/project/c3nl_djs_imaging_data/live/analysis/paeds\n",
    "design=${wd}/scripts/design/demo.mat\n",
    "contrast=${wd}/scripts/design/mainContrast.con\n",
    "\n",
    "# Run setup masks command\n",
    "setup_masks ${design} ${contrast} ${wd}/scripts/design/MNIlesion `ls ${wd}/derivatives/lesionMasks/MNI/bin*`   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$setup\"\n",
    "export setup=$1;\n",
    "source ${setup}\n",
    "echo ${workingDir}\n",
    "module load fsl/6.0.1/\n",
    "#----------------------------\n",
    "\n",
    "# Update working dir, as done after main analysis was conducted\n",
    "wd=/rds/general/project/c3nl_djs_imaging_data/live/analysis/paeds\n",
    "output=${wd}/tbss_lesion_output/\n",
    "\n",
    "# Remove pilot\n",
    "# fslmerge -t ${wd}/derivatives/volumetric_results/all_smwc1 `cat ${wd}/derivatives/volumetric_results/smwc1_list.txt`\n",
    "# fslmerge -t ${wd}/derivatives/volumetric_results/all_smwc2 `cat ${wd}/derivatives/volumetric_results/smwc2_list.txt`\n",
    "\n",
    "for ii in smwc1 smwc2\n",
    "    do\n",
    "    data_input=${wd}/derivatives/volumetric_results/all_${ii}.nii.gz\n",
    "    data_mask=${wd}/derivatives/volumetric_results/mean_${ii}_mask.nii.gz\n",
    "    design=${wd}/scripts/design/MNIlesion.mat\n",
    "    contrast=${wd}/scripts/design/MNIlesion.con\n",
    "    basename=${wd}/scripts/design/MNIlesion.nii.gz\n",
    "    \n",
    "    mkdir ${output}/${ii}\n",
    "    ## Run command ##\n",
    "    ${dep}/pbs_randomise_par -wt 24:00:00 -mem 14Gb -i ${data_input} -o ${output}/${ii}/${ii}_TBSS -m ${data_mask} -d ${design} -t ${contrast} --vxl=-4 --vxf=${basename} -n 5000 --T2 -V  \n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freesurfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$setup\"\n",
    "export setup=$1;\n",
    "source ${setup}\n",
    "echo ${workingDir}\n",
    "\n",
    "module load fsl\n",
    "#----------------------------\n",
    "\n",
    "# Set job command File \n",
    "echo -n \"\" > ${workingDir}/${project}_fs_job.txt\n",
    "job=${workingDir}/${project}_fs_job.txt\n",
    "\n",
    "\n",
    "### Job loop ###\n",
    "for subject in sub-CIF1703 sub-CIF2178 #`ls ${workingDir}/data/`\n",
    "    do\n",
    "    for ses in `ls ${workingDir}/data/${subject}/`; \n",
    "        do \n",
    "        rm -r ${workingDir}/data/${subject}/${ses}/anat/T1w/fs\n",
    "        rm -r ${workingDir}/data/${subject}/${ses}/anat/T1w/fsaverage\n",
    "        rm -r ${workingDir}/data/${subject}/${ses}/anat/T1w/T1w\n",
    "        echo \"/rds/general/user/nbourke/home/group_paeds/scripts/pbsFreesurfer -i ${subject} ${ses} ${workingDir}\" >> ${job}           \n",
    "    done\n",
    "done\n",
    "\n",
    "    # Run job\n",
    "    ${dep}/hpcSubmit ${job} 20:00:00 1 12Gb\n",
    "    echo \"\"; echo \"***\"; echo \"\"; echo \"Submitted commands:\"\n",
    "    head ${job}\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *The following aparcstats2table command works when copied into the terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$setup\"\n",
    "export setup=$1;\n",
    "source ${setup}\n",
    "echo ${workingDir}\n",
    "\n",
    "module load fsl\n",
    "module load freesurfer\n",
    "#----------------------------\n",
    "EXPERIMENT_DIR=${workingDir}\n",
    "export SUBJECTS_DIR=`${workingDir}/data/`\n",
    "\n",
    "\n",
    "counter=1\n",
    "### Job loop ###\n",
    "for subject in `ls ${workingDir}/data/`\n",
    "    do\n",
    "    for ses in `ls ${workingDir}/data/${subject}/`; \n",
    "        do  \n",
    "        echo -e \"$( if [ \"${counter}\" -eq \"1\" ]; then echo \"First run: \"; fi )${subject}\"\n",
    "\n",
    "        \n",
    "        XX=${workingDir}/data/${subject}/${ses}/anat/T1w/fs \n",
    "        #Thickness\n",
    "        aparcstats2table --subjects $XX --hemi rh --meas thickness  --tablefile ${workingDir}/data/${subject}/${ses}/anat/T1w/fs/rh_thick_aparc_stats.txt\n",
    "        aparcstats2table --subjects $XX --hemi lh --meas thickness  --tablefile ${workingDir}/data/${subject}/${ses}/anat/T1w/fs/lh_thick_aparc_stats.txt\n",
    "        # Volume\n",
    "        aparcstats2table --subjects $XX --hemi rh --meas volume --tablefile ${workingDir}/data/${subject}/${ses}/anat/T1w/fs/rh_vol_aparc_stats.txt\n",
    "        aparcstats2table --subjects $XX --hemi lh --meas volume --tablefile ${workingDir}/data/${subject}/${ses}/anat/T1w/fs/lh_vol_aparc_stats.txt\n",
    "        \n",
    "        \n",
    "    done\n",
    "    counter=$((counter +1))\n",
    "done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting FreeSurfer measurments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$setup\"\n",
    "export setup=$1;\n",
    "source ${setup}\n",
    "echo ${workingDir}\n",
    "\n",
    "#----------------------------\n",
    "\n",
    "echo \"\" > ${workingDir}/rh_thick_aparc_stats.txt\n",
    "echo \"\" > ${workingDir}/lh_thick_aparc_stats.txt\n",
    "echo \"\" > ${workingDir}/rh_vol_aparc_stats.txt\n",
    "echo \"\" > ${workingDir}/lh_vol_aparc_stats.txt\n",
    "\n",
    "counter=1\n",
    "### Job loop ###\n",
    "for subject in `ls ${workingDir}/data/`\n",
    "    do\n",
    "    for ses in `ls ${workingDir}/data/${subject}/`; \n",
    "        do  \n",
    "        XX=${workingDir}/data/${subject}/${ses}/anat/T1w/fs\n",
    "        \n",
    "        echo -e \"$( if [ \"${counter}\" -eq \"10\" ]; then sed '1q;d' ${XX}/rh_thick_aparc_stats.txt >> ${workingDir}/rh_thick_aparc_stats.txt; sed '1q;d' ${XX}/lh_thick_aparc_stats.txt >> ${workingDir}/lh_thick_aparc_stats.txt; sed '1q;d' ${XX}/rh_vol_aparc_stats.txt >> ${workingDir}/rh_vol_aparc_stats.txt; sed '1q;d' ${XX}/lh_vol_aparc_stats.txt >> ${workingDir}/lh_vol_aparc_stats.txt; fi )\"  \n",
    " \n",
    "        sed '2q;d' ${XX}/rh_thick_aparc_stats.txt >> ${workingDir}/rh_thick_aparc_stats.txt\n",
    "        sed '2q;d' ${XX}/lh_thick_aparc_stats.txt >> ${workingDir}/lh_thick_aparc_stats.txt\n",
    "        sed '2q;d' ${XX}/rh_vol_aparc_stats.txt >> ${workingDir}/rh_vol_aparc_stats.txt\n",
    "        sed '2q;d' ${XX}/lh_vol_aparc_stats.txt >> ${workingDir}/lh_vol_aparc_stats.txt\n",
    "\n",
    "    done\n",
    "    counter=$((counter +1))\n",
    "done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. mv to local\n",
    "2. open in excell\n",
    "3. correct heading\n",
    "4. save as csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering lesion masks to MNI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$setup\"\n",
    "export setup=$1;\n",
    "source ${setup}\n",
    "echo ${workingDir}\n",
    "\n",
    "#----------------------------\n",
    "\n",
    "echo \"\" > ${workingDir}/commandLogs/lesion_reg.txt\n",
    "job=${workingDir}/commandLogs/lesion_reg.txt\n",
    "mkdir ${workingDir}/tmpReg\n",
    "### Job loop ###\n",
    "for subject in `ls ${workingDir}/data/`\n",
    "    do\n",
    "    for ses in `ls ${workingDir}/data/${subject}/`; \n",
    "        do  \n",
    "        brain=${workingDir}/data/${subject}/${ses}/anat/T1w/${subject}_${ses}_T1w_brain.nii.gz  \n",
    "        lesion=${workingDir}/lesionMasks/${subject}_${ses}_contusion.nii.gz\n",
    "        \n",
    "        if [ -f \"$lesion\" ]; then\n",
    "            echo \"$ses has a lesion\"\n",
    "            echo \"${fsl}; flirt -in ${brain} -ref /rds/general/apps/fsl/5.0.10/install/data/standard/MNI152_T1_1mm_brain.nii.gz -omat ${workingDir}/tmpReg/${subject}_${ses}_T1brain2MNI.mat -dof 6 -cost mutualinfo -searchcost mutualinfo; flirt -in ${lesion} -ref /rds/general/apps/fsl/5.0.10/install/data/standard/MNI152_T1_1mm_brain.nii.gz -applyxfm -init ${workingDir}/tmpReg/${subject}_${ses}_T1brain2MNI.mat -out ${workingDir}/lesionMasks/MNI/${subject}_${ses}_contusion_MNI.nii.gz\" >> ${job}  \n",
    "        else\n",
    "            echo \"$ses does not have a lesion, making empty mask file\"\n",
    "            cp /rds/general/user/nbourke/home/templates/MNI152_T1_1mm_empty_mask.nii ${workingDir}/lesionMasks/MNI/${subject}_${ses}_empty_mask_MNI.nii.gz\n",
    "        fi\n",
    "    done\n",
    "done\n",
    "\n",
    "    # Run job\n",
    "    ${dep}/hpcSubmit ${job} 08:00:00 1 12Gb\n",
    "    echo \"\"; echo \"***\"; echo \"\"; echo \"Submitted commands:\"\n",
    "    head ${job}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
